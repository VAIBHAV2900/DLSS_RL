{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLSS RL DQN Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwb9hFvV1TTW"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nihalgeorge01/DLSS_RL/blob/main/DLSS_RL_DQN_Code.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLLkiUxgXRO2"
      },
      "source": [
        "#Colab notebook is running on a remote server so render doesn't works in it\n",
        "!apt-get install -y xvfb python-opengl  #installing xvbf dependencies and opengl\n",
        "!pip install gym pyvirtualdisplay  #Python wrapper, pyvirtualdisplay in order to interact with Xvfb virtual displays from within Python."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V75inJKiXZBO",
        "outputId": "74c73c3a-5f2e-4cf7-ec0f-0c90223f9b22"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import IPython.display as ipyd\n",
        "from pyvirtualdisplay import Display\n",
        "from time import sleep\n",
        "from PIL import Image\n",
        "display = Display(visible=False, size=(400, 300)) #to create a virtual display in the background with which Gym Envs can connect for rendering purposes\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f4b525ba090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_tgYV_HL2MX"
      },
      "source": [
        "##CartPole\n",
        "####&emsp;A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and<br>&emsp;the goal is to prevent it from falling over by increasing and reducing the cart's velocity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "JFJBHDYtcFU3",
        "outputId": "986678f6-6108-4ae2-ee62-d2641536d44b"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "screen = env.render(mode='rgb_array') #returns RGB array which can be displayed\n",
        "img = Image.fromarray(screen) #returns a image object\n",
        "ipyd.display(img) #displaying received image object through virtual display"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=600x400 at 0x7F4B5258BBD0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjkaklnxMVNI"
      },
      "source": [
        "##Actions:\n",
        "\n",
        "&emsp;Num  &ensp;&nbsp; Action<br />\n",
        "&emsp;&ensp; 0 &ensp;&emsp; Push cart to the left<br />\n",
        "&emsp;&ensp; 1 &ensp;&emsp; Push cart to the right\n",
        "\n",
        "##Reward:\n",
        "&emsp;Reward is 1 for each step you survive\n",
        "\n",
        "##Episode Termination:\n",
        "* Pole Angle is more than 15 degrees.<br>\n",
        "* Cart Position is more than 2.4 units(center of the cart reaches the edge of the display).<br>\n",
        "* Episode length is greater than 200.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "iJnU_YnvYMvY",
        "outputId": "45108485-4170-4e37-a782-0e5020697a35"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "state = env.reset()\n",
        "done = False\n",
        "total_reward=0\n",
        "while True:\n",
        "  ipyd.clear_output(wait=True) #Clear the output of the current cell receiving output. wait=True-> Wait to clear the output until new output is available to replace it. \n",
        "  ipyd.display(Image.fromarray( env.render(mode='rgb_array') )) \n",
        "  sleep(0.1)\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "  action = env.action_space.sample() #picks random action from action_space\n",
        "  next_state, reward, done, _ = env.step(action) # take action \n",
        "  total_reward +=reward\n",
        " \n",
        "print(f\"Total reward : {total_reward}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=600x400 at 0x7F4B532B0A10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward : 19.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4Nftv4DeXbm",
        "outputId": "3d9959e2-2f0c-472c-9e64-0241a0af112d"
      },
      "source": [
        "next_state"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.05902904,  0.99548067, -0.23328866, -1.96015813])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDW00C5QbGQd"
      },
      "source": [
        "###Observation_space: \n",
        "####&emsp;[ Cart Position,&nbsp; Cart Velocity,&nbsp; Pole Angle,&nbsp; Pole Angular Velocity ]\n",
        "\n",
        "###Starting State:\n",
        "####&emsp;All observations are assigned a uniform random value in [ -0.05 , 0.05 ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2sUFjE5N-0t",
        "outputId": "59f7f9bb-62e5-457c-9ee7-83aa65293ce5"
      },
      "source": [
        "env.observation_space.high"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSsQuhc6ONhs",
        "outputId": "8159fce4-b83f-45ba-9fb2-be74171992f8"
      },
      "source": [
        "env.observation_space.low"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmlLQaAri1W0"
      },
      "source": [
        "#DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkvHQT6djClS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque  #Deque is preferred over list in the cases where we need quicker append and pop operations\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eBBdU67m0rs"
      },
      "source": [
        "class replaybuffer():\n",
        "  def __init__(self,buffer_size):\n",
        "    self.buffer = deque(maxlen=buffer_size) #initilize deque that will store tuple\n",
        " \n",
        "  def add_to_buffer(self,state,action,reward,next_state,done): #add new elements in buffer\n",
        "    self.buffer.append({'state':np.array(state),\"action\":action,\"reward\":reward,\"next_state\":np.array(next_state),\"done\":done}) \n",
        " \n",
        "  def get_batch(self,batch_size): #return dictionary batch containing states,actions,rewards,next_states and dones tensor of specified batch size\n",
        "    sample = random.sample(self.buffer,batch_size)#random selections of elements from buffer\n",
        "    batch = {}\n",
        "    for key in sample[0].keys():\n",
        "      batch[key+'s']=torch.tensor([d[key] for d in sample])\n",
        "    return batch\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atQbhLrqT59h",
        "outputId": "2071ded4-df16-49ef-968b-ef3e0ee92306"
      },
      "source": [
        "try_buffer = replaybuffer(100)\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "  action = env.action_space.sample()\n",
        "  next_state,reward,done,_ = env.step(action)\n",
        "  try_buffer.add_to_buffer(state,action,reward,next_state,done)\n",
        "  state = next_state\n",
        "try_buffer.buffer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "deque([{'action': 0,\n",
              "        'done': False,\n",
              "        'next_state': array([ 0.00819095, -0.23587992, -0.00730614,  0.24736183]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([ 0.00900797, -0.04085091, -0.0064405 , -0.04328213])},\n",
              "       {'action': 0,\n",
              "        'done': False,\n",
              "        'next_state': array([ 0.00347335, -0.43089677, -0.0023589 ,  0.53773131]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([ 0.00819095, -0.23587992, -0.00730614,  0.24736183])},\n",
              "       {'action': 0,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.00514458, -0.62598548,  0.00839572,  0.82967004]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([ 0.00347335, -0.43089677, -0.0023589 ,  0.53773131])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.01766429, -0.4309793 ,  0.02498912,  0.53963938]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.00514458, -0.62598548,  0.00839572,  0.82967004])},\n",
              "       {'action': 0,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.02628388, -0.62644345,  0.03578191,  0.84009012]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.01766429, -0.4309793 ,  0.02498912,  0.53963938])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.03881275, -0.43182782,  0.05258371,  0.55887117]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.02628388, -0.62644345,  0.03578191,  0.84009012])},\n",
              "       {'action': 0,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.04744931, -0.62764691,  0.06376114,  0.8676463 ]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.03881275, -0.43182782,  0.05258371,  0.55887117])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.06000224, -0.4334478 ,  0.08111406,  0.59567265]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.04744931, -0.62764691,  0.06376114,  0.8676463 ])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.0686712 , -0.23954915,  0.09302752,  0.32960235]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.06000224, -0.4334478 ,  0.08111406,  0.59567265])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.07346218, -0.04586611,  0.09961956,  0.06764466]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.0686712 , -0.23954915,  0.09302752,  0.32960235])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.0743795 ,  0.14769694,  0.10097246, -0.19202067]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.07346218, -0.04586611,  0.09961956,  0.06764466])},\n",
              "       {'action': 0,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.07142557, -0.04871363,  0.09713204,  0.13073008]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.0743795 ,  0.14769694,  0.10097246, -0.19202067])},\n",
              "       {'action': 0,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.07239984, -0.2450831 ,  0.09974665,  0.4524078 ]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.07142557, -0.04871363,  0.09713204,  0.13073008])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.0773015 , -0.05150277,  0.1087948 ,  0.19275752]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.07239984, -0.2450831 ,  0.09974665,  0.4524078 ])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.07833156,  0.14190828,  0.11264995, -0.06372117]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.0773015 , -0.05150277,  0.1087948 ,  0.19275752])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.07549339,  0.33524998,  0.11137553, -0.31884645]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.07833156,  0.14190828,  0.11264995, -0.06372117])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.06878839,  0.52862405,  0.1049986 , -0.57443363]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.07549339,  0.33524998,  0.11137553, -0.31884645])},\n",
              "       {'action': 0,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.05821591,  0.33219892,  0.09350993, -0.2506057 ]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.06878839,  0.52862405,  0.1049986 , -0.57443363])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.05157193,  0.52586977,  0.08849781, -0.51239092]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.05821591,  0.33219892,  0.09350993, -0.2506057 ])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.04105454,  0.71964104,  0.07824999, -0.77592596]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.05157193,  0.52586977,  0.08849781, -0.51239092])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.02666172,  0.91360454,  0.06273148, -1.0429989 ]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.04105454,  0.71964104,  0.07824999, -0.77592596])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([-0.00838962,  1.10783986,  0.0418715 , -1.31534784]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.02666172,  0.91360454,  0.06273148, -1.0429989 ])},\n",
              "       {'action': 0,\n",
              "        'done': False,\n",
              "        'next_state': array([ 0.01376717,  0.91221385,  0.01556454, -1.0098594 ]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([-0.00838962,  1.10783986,  0.0418715 , -1.31534784])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([ 0.03201145,  1.10712465, -0.00463265, -1.29761439]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([ 0.01376717,  0.91221385,  0.01556454, -1.0098594 ])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([ 0.05415394,  1.30230512, -0.03058494, -1.59174395]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([ 0.03201145,  1.10712465, -0.00463265, -1.29761439])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([ 0.08020005,  1.49777647, -0.06241981, -1.89380441]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([ 0.05415394,  1.30230512, -0.03058494, -1.59174395])},\n",
              "       {'action': 0,\n",
              "        'done': False,\n",
              "        'next_state': array([ 0.11015557,  1.30338447, -0.1002959 , -1.62112378]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([ 0.08020005,  1.49777647, -0.06241981, -1.89380441])},\n",
              "       {'action': 0,\n",
              "        'done': False,\n",
              "        'next_state': array([ 0.13622326,  1.10957673, -0.13271838, -1.36131071]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([ 0.11015557,  1.30338447, -0.1002959 , -1.62112378])},\n",
              "       {'action': 1,\n",
              "        'done': False,\n",
              "        'next_state': array([ 0.1584148 ,  1.30608875, -0.15994459, -1.69239126]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([ 0.13622326,  1.10957673, -0.13271838, -1.36131071])},\n",
              "       {'action': 0,\n",
              "        'done': False,\n",
              "        'next_state': array([ 0.18453657,  1.1131348 , -0.19379242, -1.45347806]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([ 0.1584148 ,  1.30608875, -0.15994459, -1.69239126])},\n",
              "       {'action': 0,\n",
              "        'done': True,\n",
              "        'next_state': array([ 0.20679927,  0.92084794, -0.22286198, -1.22706594]),\n",
              "        'reward': 1.0,\n",
              "        'state': array([ 0.18453657,  1.1131348 , -0.19379242, -1.45347806])}])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIxgEVzKV2Hw",
        "outputId": "f8579c92-25a3-4a6d-9bb3-3b172d11b78f"
      },
      "source": [
        "batch = try_buffer.get_batch(5)\n",
        "batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'actions': tensor([1, 1, 0, 0, 1]),\n",
              " 'dones': tensor([False, False, False, False, False]),\n",
              " 'next_states': tensor([[-0.0735, -0.0459,  0.0996,  0.0676],\n",
              "         [-0.0388, -0.4318,  0.0526,  0.5589],\n",
              "         [-0.0714, -0.0487,  0.0971,  0.1307],\n",
              "         [ 0.1362,  1.1096, -0.1327, -1.3613],\n",
              "         [ 0.1584,  1.3061, -0.1599, -1.6924]], dtype=torch.float64),\n",
              " 'rewards': tensor([1., 1., 1., 1., 1.]),\n",
              " 'states': tensor([[-0.0687, -0.2395,  0.0930,  0.3296],\n",
              "         [-0.0263, -0.6264,  0.0358,  0.8401],\n",
              "         [-0.0744,  0.1477,  0.1010, -0.1920],\n",
              "         [ 0.1102,  1.3034, -0.1003, -1.6211],\n",
              "         [ 0.1362,  1.1096, -0.1327, -1.3613]], dtype=torch.float64)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5XQ9DRYi4e_"
      },
      "source": [
        "class network(nn.Module):\n",
        "  def __init__(self,input_size,output_size):\n",
        "    super(network,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size,24) #First hidden layer with 24 neurons\n",
        "    self.fc2 = nn.Linear(24,24) #Second hidden layer with 24 neurons\n",
        "    self.output_layer = nn.Linear(24,output_size) \n",
        " \n",
        "  def forward(self,input):\n",
        "    out = F.relu(self.fc1(input))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = self.output_layer(out)\n",
        " \n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHMJAu7U_jAt"
      },
      "source": [
        "class DQN_Agent():\n",
        "  def __init__(self,environment='CartPole-v0',learning_rate=0.001,start=0.9,end=0.005,decay=50,buffer_size = 10000):\n",
        "    self.eps_start=start     #epsilon at start of episode\n",
        "    self.eps_end=end         #epsilon at end of episode\n",
        "    self.eps_decay=decay     #epsilon decay rate\n",
        "    self.env = gym.make(environment)   \n",
        "    self.q_net = network(self.env.observation_space.shape[0],self.env.action_space.n)  #initilize q network \n",
        "    self.target_net = copy.deepcopy(self.q_net)  # intilize target network\n",
        "    self.Optimizer = optim.Adam(self.q_net.parameters(),lr=learning_rate ) \n",
        "    self.buffer = replaybuffer(buffer_size)  #intilize repaly buffer\n",
        "    self.criterion = nn.MSELoss()\n",
        "\n",
        "  def __select_action(self,state,episode): #selects action according to exploration-exploitation startegy\n",
        "    eps = self.eps_end + (self.eps_start-self.eps_end)*math.exp(-1.*episode/self.eps_decay) #expoential epsilon decay\n",
        "    state = torch.FloatTensor(state)\n",
        "    if random.random() > eps:   \n",
        "      with torch.no_grad():\n",
        "        return torch.argmax(self.q_net(state)).item() \n",
        "    else :\n",
        "      return torch.tensor([self.env.action_space.sample()]).item()\n",
        "  \n",
        "  def __learn(self,batch,gamma): #update network\n",
        "    self.q_net.train()  \n",
        "    rewards = batch['rewards'].reshape(len(batch['rewards']),1)  #Reshaping to 2D tensor\n",
        "    max_next_state = torch.max(self.target_net(batch['next_states'].float()).detach(),dim=1,keepdim=True)[0]\n",
        "    dones = (1-(batch['dones']).reshape(len(batch['dones']),1).to(torch.int8)) #batch['dones'] is boolen tensor, converting this to 2D int tensor\n",
        "    next_q_values = rewards + gamma*max_next_state*dones\n",
        "    actions = batch['actions'].reshape(len(batch['actions']),1).to(torch.int64) #converting to 2D tensor with dtype int64 because gather wants input as int64\n",
        "    current_q_values = self.q_net(batch['states'].float()).gather(1,actions) #gather returns elements at indexes specified in action\n",
        "    loss = self.criterion(current_q_values,next_q_values) #calculating loss\n",
        "    self.Optimizer.zero_grad() \n",
        "    loss.backward()\n",
        "    self.Optimizer.step()\n",
        "\n",
        "  def train(self,episodes,print_every=200,gamma=0.99,batch_size=128,target_update=10):  #To train network\n",
        "    for i in range(1,episodes+1):\n",
        "      state = self.env.reset()   #resetting environment at starting of each episode\n",
        "      done,total_reward,steps = False,0,0 \n",
        "      while not done :\n",
        "        action = self.__select_action(state,i) \n",
        "        next_state, reward,done,_ = self.env.step(action) \n",
        "        steps+=1\n",
        "        if done and steps<200:  #if episode terminates before 200 steps\n",
        "          reward = -1.0         #this will held network understand when it is taking wrong step\n",
        "        self.buffer.add_to_buffer(state,action,reward,next_state,done) #storing transition tuple in buffer\n",
        "        state = next_state\n",
        "        total_reward+=reward\n",
        "      if len(self.buffer)>batch_size:  \n",
        "        self.__learn(self.buffer.get_batch(batch_size),gamma) #updating network\n",
        "      if not i%target_update:  #update target network\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict()) \n",
        "      if not i%print_every :  \n",
        "        if i==episodes:\n",
        "          print(\"Finished training ðŸ™Œ\")\n",
        "        else:\n",
        "          print(f\"Training...........Finished episode:{i} Total reward earned for this episode:{total_reward}\")\n",
        "          total_reward = 0\n",
        "  \n",
        "  def play(self): \n",
        "    state = self.env.reset()\n",
        "    total_reward,done,steps = 0, False,0\n",
        "    while True:    \n",
        "      img = Image.fromarray( self.env.render(mode='rgb_array') )\n",
        "      ipyd.clear_output(wait=True)\n",
        "      ipyd.display(img)\n",
        "      sleep(0.1) \n",
        "      if done:\n",
        "        break\n",
        "      action = torch.argmax(self.q_net(torch.tensor(state).float())).item()\n",
        "      next_state, reward , done , _ = self.env.step(action)\n",
        "      total_reward+=reward\n",
        "      steps+=1 \n",
        "      state = next_state\n",
        "    print(\"Total reward:\",total_reward)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "wlKJ5xCYC6k7",
        "outputId": "1c659923-d28c-463f-a7db-b544bc8be452"
      },
      "source": [
        "agent = DQN_Agent()\n",
        "agent.play()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=600x400 at 0x7F4B53315710>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward: 12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpP-H1v-EwNN",
        "outputId": "f254fe3f-6aca-4837-dde5-c87e40976d16"
      },
      "source": [
        "agent.train(episodes=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...........Finished episode:200 Total reward earned for this episode:8.0\n",
            "Training...........Finished episode:400 Total reward earned for this episode:16.0\n",
            "Training...........Finished episode:600 Total reward earned for this episode:47.0\n",
            "Training...........Finished episode:800 Total reward earned for this episode:36.0\n",
            "Training...........Finished episode:1000 Total reward earned for this episode:26.0\n",
            "Training...........Finished episode:1200 Total reward earned for this episode:64.0\n",
            "Training...........Finished episode:1400 Total reward earned for this episode:42.0\n",
            "Training...........Finished episode:1600 Total reward earned for this episode:35.0\n",
            "Training...........Finished episode:1800 Total reward earned for this episode:56.0\n",
            "Training...........Finished episode:2000 Total reward earned for this episode:48.0\n",
            "Training...........Finished episode:2200 Total reward earned for this episode:200.0\n",
            "Training...........Finished episode:2400 Total reward earned for this episode:200.0\n",
            "Training...........Finished episode:2600 Total reward earned for this episode:192.0\n",
            "Training...........Finished episode:2800 Total reward earned for this episode:39.0\n",
            "Training...........Finished episode:3000 Total reward earned for this episode:179.0\n",
            "Training...........Finished episode:3200 Total reward earned for this episode:200.0\n",
            "Training...........Finished episode:3400 Total reward earned for this episode:200.0\n",
            "Training...........Finished episode:3600 Total reward earned for this episode:99.0\n",
            "Training...........Finished episode:3800 Total reward earned for this episode:31.0\n",
            "Training...........Finished episode:4000 Total reward earned for this episode:65.0\n",
            "Training...........Finished episode:4200 Total reward earned for this episode:200.0\n",
            "Training...........Finished episode:4400 Total reward earned for this episode:141.0\n",
            "Training...........Finished episode:4600 Total reward earned for this episode:190.0\n",
            "Training...........Finished episode:4800 Total reward earned for this episode:166.0\n",
            "Finished training ðŸ™Œ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "Fu1TnAubGxBl",
        "outputId": "cbcea1cd-8329-4c26-8d35-3cb4d9444d9c"
      },
      "source": [
        "agent.play()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=600x400 at 0x7F4B52197490>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward: 200.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
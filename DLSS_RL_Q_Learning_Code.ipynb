{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evFOeZcz0oKg"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nihalgeorge01/DLSS_RL/blob/main/DLSS_RL_Q_Learning_Code.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17qClfwfYbbf",
    "outputId": "ffbcf039-9f18-48e2-c32e-823f3afdc980"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "\n",
      "491\n",
      "Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np  \n",
    "# Create an environment of Taxi-v3:\n",
    "env = gym.make('Taxi-v3').env \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkKcgxJrYeNo",
    "outputId": "70e06c75-1f90-4fcb-ecce-f6b41ae58c10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 4304\n",
      "Penalties incurred: 2381\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "epochs = 0\n",
    "penalty, reward = 0, 0  # records the number of times the agent hits a wall\n",
    "frames = []\n",
    "done = False # tracks whether episode is finished or not\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if new_state == state:\n",
    "      penalty+=1\n",
    "    state = new_state\n",
    "\n",
    "    frames.append({'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward})\n",
    "    epochs += 1\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "6izwZy86YnU3",
    "outputId": "f33315b0-fffd-46a9-f265-00881d07fe2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "Timestep: 29\n",
      "State: 349\n",
      "Action: 2\n",
      "Reward: -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3e35c310943e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reward: {frame['reward']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-3e35c310943e>\u001b[0m in \u001b[0;36mprint_frames\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Action: {frame['action']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reward: {frame['reward']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait = True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i+1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(1)\n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do5Rw7aXYvCC"
   },
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sdv7DMW6YvTU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "(alpha, gamma, episodes, epsilon) = (0.6, 0.9, 1000, 0.4)\n",
    "total_epochs =0\n",
    "total_penalty = 0\n",
    "illegal_episode =[]\n",
    "for i in range(episodes):\n",
    "  state = env.reset()\n",
    "  epochs = 0\n",
    "  penalty, reward_tot = 0, 0  \n",
    "  illegal =0\n",
    "  \n",
    "  done = False \n",
    "  while not done:\n",
    "    rand = random.uniform(0,1)\n",
    "    if rand < epsilon:\n",
    "      action = env.action_space.sample()\n",
    "    if rand >= epsilon:\n",
    "      action = np.argmax(q_table[state])\n",
    "\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if new_state == state:\n",
    "      penalty+=1\n",
    "\n",
    "    if reward == -10:\n",
    "      illegal+=1\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "    #Q_cal = Reward + gamma*max{a}(Q(s',.))\n",
    "    #new_q = oldq + aplha(Q_cal - oldq) = (1-alpha)oldq + alpha(Q_cal)\n",
    "          \n",
    "\n",
    "    oldq = q_table[state, action]\n",
    "    new_state_max = np.max(q_table[new_state]) \n",
    "    newq = (1 - alpha) * oldq + alpha * (reward + gamma * new_state_max)\n",
    "    q_table[state, action] = newq\n",
    "    \n",
    "    state = new_state\n",
    "    epochs+=1\n",
    "  illegal_episode.append(illegal)\n",
    "  total_penalty+=penalty\n",
    "  total_epochs+=epochs\n",
    "\n",
    "total_penalty/=episodes\n",
    "total_epochs/=episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_AjBNluZCVc",
    "outputId": "f21c9cab-c139-4e99-c227-9cd4d3dbba15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 12\n",
      "Penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "epochs = 0\n",
    "penalty, reward = 0, 0  \n",
    "frames = []\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state,:])\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if new_state == state:\n",
    "      penalty+=1\n",
    "    state = new_state\n",
    "\n",
    "    frames.append({'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward})\n",
    "    epochs += 1\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmWhynDyZQOh",
    "outputId": "050ca58e-d890-449b-c9a8-1356926ed1fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 12\n",
      "State: 85\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DLSS RL Q-Learning Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
